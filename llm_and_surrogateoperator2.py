# -*- coding: utf-8 -*-
"""LLM_and_SurrogateOperator2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P2TUrHZ6lheBbEC9plVMeIZSepHnwDUa
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers torch ctranslate2

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=False)
model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=False)

import re, json
from typing import Dict, Any

# --- エイリアス定義（必要に応じて拡張してください） ---
direction_mapping = {
    "exhaust": ["排気側", "排気", "exhaust", "outlet"],
    "intake":  ["吸気側", "吸気", "intake", "inlet"],
    "front":   ["フロント側", "フロント", "front"],
    "rear":    ["リア側", "リア", "rear", "back"]
}
ALL_ALIASES = ["全部", "全て", "すべて", "all", "overall", "共通", "同じ"]

RE_NUMBER = r"(-?\d+(?:\.\d+)?)"

def _find_number_near(text: str, start_idx: int, window: int = 25):
    seg = text[start_idx:start_idx+window]
    m = re.search(rf"{RE_NUMBER}\s*(?:度|℃|°C|C)?", seg, flags=re.IGNORECASE)
    return float(m.group(1)) if m else None

def _parse_global_default(text: str):
    pat = r"(?:{}).*?{}\s*(?:度|℃|°C|C)?".format("|".join(map(re.escape, ALL_ALIASES)), RE_NUMBER)
    m = re.search(pat, text, flags=re.IGNORECASE | re.DOTALL)
    return float(m.group(1)) if m else None

def parse_free_text_to_dict(text: str, direction_mapping: Dict[str, list]) -> Dict[str, float]:
    """自由文から各部位温度を辞書化（数値: °C）。"""
    result = {}
    g = _parse_global_default(text)

    # 明示指定
    for key, aliases in direction_mapping.items():
        for alias in sorted(aliases, key=len, reverse=True):
            for m in re.finditer(re.escape(alias), text, flags=re.IGNORECASE):
                temp = _find_number_near(text, m.end(), window=25)
                if temp is not None:
                    result[key] = temp
                    break
            if key in result:
                break

    # グローバル既定で埋める
    if g is not None:
        for key in direction_mapping.keys():
            if key not in result:
                result[key] = g
    return result

def _ensure_all_keys(d: Dict[str, Any]) -> Dict[str, float]:
    """不足キーを問い合わせて補完（コンソール利用）。"""
    out = dict(d)
    for key, aliases in direction_mapping.items():
        if key not in out:
            terms = ", ".join(aliases)
            while True:
                v = input(f"{terms} の目標温度(℃)を入力してください: ")
                try:
                    out[key] = float(v)
                    break
                except ValueError:
                    print("数値で入力してください（例: 210）")
    return out

# ------------------ LLM 抽出（JSON専用） ------------------
def extract_with_llm_to_json(model, tokenizer, user_text: str) -> str:
    """
    LLMに JSON だけを出力させる。
    期待JSON: {"exhaust": 280, "intake": 220, "front": 210, "rear": 210}
    """
    system_rules = (
        "You are a strict JSON formatter. "
        "Output ONLY a minified JSON with four numeric °C values: "
        '{"exhaust": <float>, "intake": <float>, "front": <float>, "rear": <float>}. '
        "No comments, no prose, no markdown."
    )
    # Few-shot の例で形式を強化
    examples = [
        ("全部210、ただし排気は280", '{"exhaust":280,"intake":210,"front":210,"rear":210}'),
        ("front205C, rear 210, 吸気220, 排気280℃", '{"exhaust":280,"intake":220,"front":205,"rear":210}'),
    ]

    prompt = ""
    prompt += f"[SYSTEM]\n{system_rules}\n"
    for q, a in examples:
        prompt += f"[USER]\n{q}\n[ASSISTANT]\n{a}\n"
    prompt += f"[USER]\n{user_text}\n[ASSISTANT]\n"

    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False, eos_token_id=tokenizer.eos_token_id)
    raw = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # 末尾の最新回答のみ取り出し（念のため最後の { から } まで抽出）
    m = re.search(r"\{.*\}", raw, flags=re.DOTALL)
    if not m:
        raise ValueError("LLMからJSONが取得できませんでした")
    js = m.group(0)

    # 構文チェック & 型厳格化
    data = json.loads(js)
    # 必須キーの存在と数値化
    for k in ["exhaust", "intake", "front", "rear"]:
        if k not in data:
            raise ValueError(f"キー {k} が不足")
        data[k] = float(data[k])
    return json.dumps(data, ensure_ascii=False, separators=(",", ":"))

# ------------------ 統合I/O ------------------
def prompt_and_return_json(model, tokenizer) -> str:
    """
    1) 自由入力を受け取る
    2) LLMでJSON抽出を試みる
    3) 失敗時はローカル正規表現→不足分を対話で補完→JSON化
    4) JSON文字列（minified）を返す
    """
    print("どこの部位を何℃に設定しますか？自然文でまとめて入力OKです。")
    print("例: 『排気280℃、吸気220、front 205C、rearは210』 / 『全部210』 / 『全部210、ただし排気は280』")
    user_text = input(">> ")

    # まず LLM で厳格JSON抽出
    try:
        js = extract_with_llm_to_json(model, tokenizer, user_text)
        print(js)  # 仕様: 出力はJSON
        return js
    except Exception as e:
        # フォールバックへ
        parsed = parse_free_text_to_dict(user_text, direction_mapping)
        parsed = _ensure_all_keys(parsed)
        js = json.dumps(
            {
                "exhaust": float(parsed["exhaust"]),
                "intake":  float(parsed["intake"]),
                "front":   float(parsed["front"]),
                "rear":    float(parsed["rear"]),
            },
            ensure_ascii=False, separators=(",", ":")
        )
        print(js)
        return js

# ---- 実行例 ----
json_result = prompt_and_return_json(model, tokenizer)
print("JSON結果:", json_result)

with open('input.json', 'w') as f:
    f.write(json_result)